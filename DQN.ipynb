{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPosP9MW8vAGJhoF6iQuDFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udsey/SATO_RL/blob/main/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRNPCMKZmOvD"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from collections import deque"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LipngijEURJh"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS6pn3gNUODH"
      },
      "source": [
        "class Train:\n",
        "\n",
        "\n",
        "    def __init__(self, time_limit=2000, v_limit=100, s_limit=300, dt=10, v_delta=1):\n",
        "        self.time_limit = time_limit # s\n",
        "        self.v_limit = v_limit # km/h\n",
        "        self.s_limit = s_limit # km\n",
        "        self.state_space = np.arange(0, self.s_limit, 0.001) # m\n",
        "        self.action_space = np.arange(-0.4, 0.5, 0.1) # m/s^2\n",
        "        self.action_space_high = 0.5\n",
        "        self.action_space_low = -0.5\n",
        "        self.dt = dt / 3600 # h\n",
        "        self.df_limit = pd.DataFrame({'start': [0],\n",
        "                                      'stop': [self.s_limit],\n",
        "                                      'limit': self.v_limit,})\n",
        "        self.v_delta = v_delta\n",
        "        self.state_dim = 3\n",
        "        self.action_dim = 1\n",
        "\n",
        "# Добавление ограничений скорости на участке\n",
        "\n",
        "    def append_velocity_limit(self, limit, start, stop):\n",
        "        self.df_limit = self.df_limit.append({'start': start,\n",
        "                                              'stop': stop,\n",
        "                                              'limit': limit,}, ignore_index=True)\n",
        "        return self.df_limit\n",
        "# Сброс скоростного ограничения к одному на всем участке\n",
        "    def reset_velocity_limit(self):\n",
        "        self.append_velocity_limit(self, limit=self.v_limit, \n",
        "                                   position=range(0, self.s_limit))\n",
        "# Сброс окружения к начальному значению\n",
        "    def reset(self):\n",
        "        self.s = 0\n",
        "        self.v = 0\n",
        "        self.a = 0\n",
        "        self.done = False\n",
        "        self.total_time = 0\n",
        "        self.reward = 0\n",
        "        self.v_list = []\n",
        "        self.s_list = []\n",
        "        self.t_list = []\n",
        "        self.a_list = []\n",
        "        self.reward_list = []\n",
        "\n",
        "        return self.return_state()\n",
        "\n",
        "# Возвращает текущее состояние\n",
        "    def return_state(self):\n",
        "        return np.array([self.s*1000, self.v, self.speed_limit()])\n",
        "\n",
        "# Возвращает скоростное ограничение на текущем участке \n",
        "    def speed_limit(self):\n",
        "        if self.s < 0:\n",
        "            return self.df_limit.iloc[0, 2]\n",
        "    \n",
        "        for i in range(self.df_limit.shape[0]):\n",
        "            min_p = self.df_limit.iloc[i, 0]\n",
        "            max_p = self.df_limit.iloc[i, 1]\n",
        "            if min_p <= self.s <= max_p:\n",
        "                return self.df_limit.iloc[i, 2]\n",
        "            else:\n",
        "                return min_p\n",
        "# Возвращает награду в соответствии с текущим состоянием\n",
        "    def reward_func(self):\n",
        "        if self.s < 0: # Уехал в обратную сторону\n",
        "            return -1000, True\n",
        "        if self.s >= self.s_limit: # Доехал до точки назначения\n",
        "            self.s = self.s_limit\n",
        "            return 100, True\n",
        "        if self.total_time > self.time_limit/3600: # Превысил время\n",
        "            return -100, True\n",
        "        if self.v > self.speed_limit()*self.v_delta: # Превысил скорость\n",
        "            return -500, True\n",
        "        if self.speed_limit()/2 <= self.v <= self.speed_limit():\n",
        "            return 1, False\n",
        "        else:\n",
        "            return 0, False\n",
        "        \n",
        "        \n",
        "\n",
        "# Шаг для заданного действия\n",
        "    def step(self, action):\n",
        "        \n",
        "        self.total_time += self.dt # h\n",
        "        self.a = action * (3600**2 / 1000) # km/h^2\n",
        "        self.s += self.v * self.dt + (self.a * (self.dt ** 2))/2 #km\n",
        "        self.v += (self.a * self.dt) # km/h\n",
        "\n",
        "        self.reward, self.done = self.reward_func()\n",
        "\n",
        "        self.v_list.append(self.v)\n",
        "        self.t_list.append(self.total_time)\n",
        "        self.s_list.append(self.s)\n",
        "        self.a_list.append(self.a)\n",
        "        self.reward_list.append(self.reward)\n",
        "        return  self.return_state(), self.reward, self.done\n",
        "\n",
        "# Справочная информация о движении\n",
        "    def action_info(self, action):\n",
        "        print('*'*20)\n",
        "        print('Ускорение {:.2f} м/с^2'.format(action))\n",
        "        print('Скорость {:.2f} км/ч' .format(self.v)),\n",
        "        print('Пройденный путь {:.6f} км'.format(self.s))\n",
        "        print('Штраф', self.reward)\n",
        "        print('*'*20)\n",
        "\n",
        "# Возвращает данные о скорости на каждом участке        \n",
        "    def speed_legend(self):\n",
        "        df = pd.DataFrame(columns=['time', 'position', 'speed', 'acceleration', 'reward'])\n",
        "        df.time = self.t_list\n",
        "        df.position = self.s_list\n",
        "        df.speed = self.v_list\n",
        "        df.acceleration = self.a_list\n",
        "        df.reward = self.reward_list\n",
        "        return df\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc7AMAf8UUvk"
      },
      "source": [
        "# DQN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC3juXcmqV0u"
      },
      "source": [
        "class Buffer():\n",
        "\n",
        "    def __init__(self, buffer_size):\n",
        "\n",
        "        self.state_buf = deque(maxlen=buffer_size)\n",
        "        self.next_state_buf = deque(maxlen=buffer_size)\n",
        "        self.action_buf = deque(maxlen=buffer_size)\n",
        "        self.reward_buf = deque(maxlen=buffer_size)\n",
        "        self.done_buf = deque(maxlen=buffer_size)\n",
        "        self.buffer_size = 0\n",
        "\n",
        "    def add(self, state, next_state, action, reward, done):\n",
        "\n",
        "        self.state_buf.append(state)\n",
        "        self.next_state_buf.append(next_state)\n",
        "        self.action_buf.append(action)\n",
        "        self.reward_buf.append(reward)\n",
        "        self.done_buf.append(done)\n",
        "        self.buffer_size +=1\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \n",
        "        indexes = np.random.randint(self.buffer_size, size=batch_size)\n",
        "        mb_state = []\n",
        "        mb_next_state = []\n",
        "        mb_action = []\n",
        "        mb_reward = []\n",
        "        mb_done = []\n",
        "\n",
        "        for i in indexes:\n",
        "            mb_state.append(self.state_buf[i])\n",
        "            mb_next_state.append(self.next_state_buf[i])\n",
        "            mb_action.append(self.action_buf[i])\n",
        "            mb_reward.append(self.reward_buf[i])\n",
        "            mb_done.append(self.done_buf[i])\n",
        "\n",
        "        return mb_state, mb_next_state, mb_action, mb_reward, mb_done\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.buffer_size"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yRnOvYPmiUE"
      },
      "source": [
        "def actor_model(state_dim, action_dim, initializer):\n",
        "\n",
        "    model = Sequential([layers.InputLayer(input_shape=(state_dim,)),\n",
        "                        layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "                        layers.Dense(128, activation='relu'),\n",
        "                        layers.Dropout(.1),\n",
        "                        layers.Dense(action_dim)])\n",
        "    return model"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Shba79znYRoh"
      },
      "source": [
        "def e_greedy(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.uniform(action_space_low, action_space_high)\n",
        "    else:\n",
        "        state = np.expand_dims(state, 0)\n",
        "        y = online_model(state).numpy()[0][0]\n",
        "        # np.argmax(action_values)\n",
        "        return y\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHDgly-Qi5wu"
      },
      "source": [
        "def q_target_val(batch):\n",
        "    mb_state, mb_next_state, mb_action, mb_reward, mb_done = batch\n",
        "    ys = []\n",
        "    for r, d, next_state in zip(mb_reward, mb_done, mb_next_state):\n",
        "        if d == True:\n",
        "            ys.append(r)\n",
        "        else:\n",
        "            next_state = np.expand_dims(next_state, 0)\n",
        "            av = target_model(next_state)\n",
        "            q_step = r + gamma*av\n",
        "            ys.append(q_step)\n",
        "    return ys\n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_BF2Fn_oMrU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "431cfbbd-5c75-4b97-f016-9f70c2596987"
      },
      "source": [
        "'''\n",
        "def test_agent(env, ??, episodes=10):\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "\n",
        "            action = e_greedy(epsilon)\n",
        "            next_state, reward, done = env.step(action)\n",
        "            buffer.add(state, next_state, action, reward, done)\n",
        "            \n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "\n",
        "        reward_list.append(total_reward)\n",
        "    return reward_list\n",
        "    '''"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef test_agent(env, ??, episodes=10):\\n    for episode in range(episodes):\\n\\n        state = env.reset()\\n        done = False\\n        total_reward = 0\\n\\n        while not done:\\n\\n            action = e_greedy(epsilon)\\n            next_state, reward, done = env.step(action)\\n            buffer.add(state, next_state, action, reward, done)\\n            \\n            total_reward += reward\\n            state = next_state\\n\\n        reward_list.append(total_reward)\\n    return reward_list\\n    '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFqHW99pZ8w7"
      },
      "source": [
        "env = Train()\n",
        "state_dim = env.state_dim\n",
        "action_dim = env.action_dim\n",
        "action_space_low = env.action_space_low\n",
        "action_space_high = env.action_space_high\n",
        "buffer_size = 1000\n",
        "batch_size = 10\n",
        "episodes = 10\n",
        "epsilon = 0.1\n",
        "gamma = 0.3\n",
        "end_explor = 1\n",
        "eps_decay = 0.001\n",
        "min_b_size = 10\n",
        "update_freq = 20\n",
        "learning_rate = 0.001\n",
        "start_explor = 1\n",
        "end_explor = 0.4\n",
        "explor_steps = 10\n",
        "num_epochs = 1000\n",
        "update_target_net = 2\n",
        "reward_list = []\n",
        "initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY7EIhqr_frE"
      },
      "source": [
        "def optimize(model, inputs, outputs):\n",
        "    loss_fn = lambda: tf.keras.losses.mse(model(input), output)\n",
        "    var_list_fn = lambda: model.trainable_weights\n",
        "    for input, output in zip(inputs, outputs):\n",
        "        opt.minimize(loss_fn, var_list_fn)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsezxAYIny3H"
      },
      "source": [
        "online_model = actor_model(state_dim, action_dim, initializer)\n",
        "target_model = actor_model(state_dim, action_dim, initializer)\n",
        "buffer = Buffer(buffer_size=buffer_size)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "OyeK_KvBgbwL",
        "outputId": "e933364d-8661-4321-ff0c-516484286902"
      },
      "source": [
        "step_count = 0\n",
        "last_update_loss = []\n",
        "batch_reward = []\n",
        "old_step_count = 0\n",
        "\n",
        "state = env.reset()\n",
        "buffer = Buffer(buffer_size=buffer_size)\n",
        "initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)\n",
        "online_model = actor_model(state_dim, action_dim, initializer)\n",
        "target_model = actor_model(state_dim, action_dim, initializer)\n",
        "epsilon = start_explor\n",
        "eps_decay = (start_explor - end_explor) / explor_steps\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        if epsilon > end_explor:\n",
        "            epsilon -= eps_decay\n",
        "\n",
        "        action = e_greedy(state, epsilon)\n",
        "        next_state, reward, done = env.step(action)\n",
        "        buffer.add(state, next_state, action, reward, done)\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "        step_count += 1\n",
        "\n",
        "        if (len(buffer) > min_b_size) and (step_count % update_freq == 0):\n",
        "            batch = buffer.sample(batch_size)\n",
        "            mb_action = batch[2]\n",
        "            y_r = q_target_val(batch)\n",
        "            train_loss = optimize(online_model, y_r, mb_action)\n",
        "            last_update_loss(train_loss)\n",
        "        \n",
        "        if (len(buffer) > min_b_size) and (step_count % update_target_net == 0):\n",
        "            target_model.set_weights(online_model.get_weights())\n",
        "\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            batch_reward.append(total_reward)\n",
        "            total_reward = 0\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-26bf136cd43a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mmb_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0my_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_target_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmb_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mlast_update_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-60c2b3329528>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(model, inputs, outputs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvar_list_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \"\"\"\n\u001b[1;32m    528\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 529\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    530\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m           \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m           \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-56-60c2b3329528>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvar_list_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         training=training_mode):\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m       \u001b[0minput_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0meager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0mcall_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    253\u001b[0m               \u001b[0;34m' incompatible with the layer: expected axis '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m               \u001b[0;34m' of input shape to have value '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m               ' but received input with shape ' + display_shape(x.shape))\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;31m# Check shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer sequential_40 is incompatible with the layer: expected axis -1 of input shape to have value 3 but received input with shape (1, 1)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF0-LjXzERcG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}