{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TD3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP3SXde9zlEyY3EQO1JEBrL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udsey/SATO_RL/blob/main/TD3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WP0pYrJJdlU-"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPmUS5NOdoEq"
      },
      "source": [
        "def limit_plot(df, return_df = False):\n",
        "    import plotly.graph_objects as go\n",
        "    df_lim = df.copy()\n",
        "    df_lim['x'] = df_lim.apply(lambda x: np.arange(x[0], x[1], 0.001), axis=1)\n",
        "    df_lim = df_lim.explode('x')\n",
        "    df_lim.drop(columns=df_lim.columns[0:2], inplace=True)\n",
        "    df_lim.rename(columns={df_lim.columns[0]:'y'}, inplace=True)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=df_lim.x, y=df_lim.y, name='speed limit'))\n",
        "    fig.show()\n",
        "    if return_df == True:\n",
        "        return df_lim\n",
        "def plot_stat(hist, total_reward_list):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(hist)\n",
        "    plt.title('Iteration number') \n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(total_reward_list)\n",
        "    plt.title('Total reward')      \n",
        "\n",
        "def plot_result(df):\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=df_limit_speed.x, y=df_limit_speed.y, name='speed limit'))\n",
        "    fig.add_trace(go.Scatter(x = df.position, y = df.speed, name='train movement'))\n",
        "    #fig.show('svg')\n",
        "    fig.show()\n",
        "\n",
        "def min_time(s, v, a=0.5):\n",
        "    s*=1000\n",
        "    v = v / 3.6\n",
        "    t = v/a\n",
        "    s_0 = a*t**2/2\n",
        "    s_ost = s-s_0\n",
        "    t_1 = s_ost/v\n",
        "    t_min  = t+t_1\n",
        "    return t_min"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3KfiKdYdps-"
      },
      "source": [
        "class Train:\n",
        "\n",
        "\n",
        "    def __init__(self, time_limit=2000, v_limit=100, s_limit=300, dt=10, v_delta=1):\n",
        "        self.time_limit = time_limit # s\n",
        "        self.v_limit = v_limit # km/h\n",
        "        self.s_limit = s_limit # km\n",
        "        self.state_space = np.arange(0, self.s_limit, 0.001) # m\n",
        "        self.action_space = np.arange(-0.4, 0.5, 0.1) # m/s^2\n",
        "        self.action_space_high = 0.5\n",
        "        self.action_space_low = -0.5\n",
        "        self.dt = dt / 3600 # h\n",
        "        self.df_limit = pd.DataFrame({'start': [0],\n",
        "                                      'stop': [self.s_limit],\n",
        "                                      'limit': self.v_limit,})\n",
        "        self.v_delta = v_delta\n",
        "\n",
        "# Добавление ограничений скорости на участке\n",
        "\n",
        "    def append_velocity_limit(self, limit, start, stop):\n",
        "        self.df_limit = self.df_limit.append({'start': start,\n",
        "                                              'stop': stop,\n",
        "                                              'limit': limit,}, ignore_index=True)\n",
        "        return self.df_limit\n",
        "# Сброс скоростного ограничения к одному на всем участке\n",
        "    def reset_velocity_limit(self):\n",
        "        self.append_velocity_limit(self, limit=self.v_limit, \n",
        "                                   position=range(0, self.s_limit))\n",
        "# Сброс окружения к начальному значению\n",
        "    def reset(self):\n",
        "        self.s = 0\n",
        "        self.v = 0\n",
        "        self.a = 0\n",
        "        self.done = False\n",
        "        self.total_time = 0\n",
        "        self.reward = 0\n",
        "        self.v_list = []\n",
        "        self.s_list = []\n",
        "        self.t_list = []\n",
        "        self.a_list = []\n",
        "        self.reward_list = []\n",
        "        self.state = [self.s*1000, self.v, self.total_time]\n",
        "        #self.state = self.s*1000\n",
        "        return self.state\n",
        "\n",
        "# Возвращает скоростное ограничение на текущем участке \n",
        "    def speed_limit(self):\n",
        "        if self.s < 0:\n",
        "            return self.df_limit.iloc[0, 2]\n",
        "    \n",
        "        for i in range(self.df_limit.shape[0]):\n",
        "            min_p = self.df_limit.iloc[i, 0]\n",
        "            max_p = self.df_limit.iloc[i, 1]\n",
        "            if min_p <= self.s <= max_p:\n",
        "                return self.df_limit.iloc[i, 2]\n",
        "            else:\n",
        "                return min_p\n",
        "# Возвращает награду в соответствии с текущим состоянием\n",
        "    def reward_func(self):\n",
        "        if self.s < 0: # Уехал в обратную сторону\n",
        "            return -1000, True\n",
        "        if self.s >= self.s_limit: # Доехал до точки назначения\n",
        "            self.s = self.s_limit\n",
        "            return 100, True\n",
        "        if self.total_time > self.time_limit/3600: # Превысил время\n",
        "            return -100, True\n",
        "        if self.v > self.speed_limit()*self.v_delta: # Превысил скорость\n",
        "            return -500, True\n",
        "        if self.speed_limit()/2 <= self.v <= self.speed_limit():\n",
        "            return 1, False\n",
        "        else:\n",
        "            return 0, False\n",
        "        \n",
        "        \n",
        "\n",
        "# Шаг для заданного действия\n",
        "    def step(self, action):\n",
        "        \n",
        "        self.total_time += self.dt # h\n",
        "        self.a = action * (3600**2 / 1000) # km/h^2\n",
        "        self.s += self.v * self.dt + (self.a * (self.dt ** 2))/2 #km\n",
        "        self.v += (self.a * self.dt) # km/h\n",
        "\n",
        "        self.reward, self.done = self.reward_func()\n",
        "\n",
        "        self.v_list.append(self.v)\n",
        "        self.t_list.append(self.total_time)\n",
        "        self.s_list.append(self.s)\n",
        "        self.a_list.append(self.a)\n",
        "        self.reward_list.append(self.reward)\n",
        "        self.state = [self.s*1000, self.v, self.total_time]\n",
        "        #self.state = self.s*1000\n",
        "        return  self.state, self.reward, self.done\n",
        "\n",
        "# Справочная информация о движении\n",
        "    def action_info(self, action):\n",
        "        print('*'*20)\n",
        "        print('Ускорение {:.2f} м/с^2'.format(action))\n",
        "        print('Скорость {:.2f} км/ч' .format(self.v)),\n",
        "        print('Пройденный путь {:.6f} км'.format(self.s))\n",
        "        print('Штраф', self.reward)\n",
        "        print('*'*20)\n",
        "\n",
        "# Возвращает данные о скорости на каждом участке        \n",
        "    def speed_legend(self):\n",
        "        df = pd.DataFrame(columns=['time', 'position', 'speed', 'acceleration', 'reward'])\n",
        "        df.time = self.t_list\n",
        "        df.position = self.s_list\n",
        "        df.speed = self.v_list\n",
        "        df.acceleration = self.a_list\n",
        "        df.reward = self.reward_list\n",
        "        return df\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDXKCPIudseE"
      },
      "source": [
        "class OUActionNoise:\n",
        "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
        "        self.theta = theta\n",
        "        self.mean = mean\n",
        "        self.std_dev = std_deviation\n",
        "        self.dt = dt\n",
        "        self.x_initial = x_initial\n",
        "        self.reset()\n",
        "\n",
        "    def __call__(self):\n",
        "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
        "        x = (\n",
        "            self.x_prev\n",
        "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
        "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
        "        )\n",
        "        # Store x into x_prev\n",
        "        # Makes next noise dependent on current one\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "\n",
        "    def reset(self):\n",
        "        if self.x_initial is not None:\n",
        "            self.x_prev = self.x_initial\n",
        "        else:\n",
        "            self.x_prev = np.zeros_like(self.mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8typyUadvko"
      },
      "source": [
        "class Buffer:\n",
        "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
        "        # Number of \"experiences\" to store at max\n",
        "        self.buffer_capacity = buffer_capacity\n",
        "        # Num of tuples to train on.\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Its tells us num of times record() was called.\n",
        "        self.buffer_counter = 0\n",
        "\n",
        "        # Instead of list of tuples as the exp.replay concept go\n",
        "        # We use different np.arrays for each tuple element\n",
        "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
        "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
        "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
        "\n",
        "    # Takes (s,a,r,s') obervation tuple as input\n",
        "    def record(self, obs_tuple):\n",
        "        # Set index to zero if buffer_capacity is exceeded,\n",
        "        # replacing old records\n",
        "        index = self.buffer_counter % self.buffer_capacity\n",
        "        \n",
        "        self.state_buffer[index] = obs_tuple[0]\n",
        "        self.action_buffer[index] = obs_tuple[1]\n",
        "        self.reward_buffer[index] = obs_tuple[2]\n",
        "        self.next_state_buffer[index] = obs_tuple[3]\n",
        "\n",
        "        self.buffer_counter += 1\n",
        "\n",
        "    # Eager execution is turned on by default in TensorFlow 2. Decorating with tf.function allows\n",
        "    # TensorFlow to build a static graph out of the logic and computations in our function.\n",
        "    # This provides a large speed up for blocks of code that contain many small TensorFlow operations such as this one.\n",
        "    @tf.function\n",
        "    def update(\n",
        "        self, state_batch, action_batch, reward_batch, next_state_batch,\n",
        "    ):\n",
        "        # Training and updating Actor & Critic networks.\n",
        "        # See Pseudo Code.\n",
        "        with tf.GradientTape() as tape:\n",
        "            target_actions = target_actor(next_state_batch, training=True)\n",
        "            y = reward_batch + gamma * target_critic(\n",
        "                [next_state_batch, target_actions], training=True\n",
        "            )\n",
        "            critic_value = critic_model([state_batch, action_batch], training=True)\n",
        "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
        "\n",
        "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
        "        critic_optimizer.apply_gradients(\n",
        "            zip(critic_grad, critic_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions = actor_model(state_batch, training=True)\n",
        "            critic_value = critic_model([state_batch, actions], training=True)\n",
        "            # Used `-value` as we want to maximize the value given\n",
        "            # by the critic for our actions\n",
        "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
        "\n",
        "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
        "        actor_optimizer.apply_gradients(\n",
        "            zip(actor_grad, actor_model.trainable_variables)\n",
        "        )\n",
        "\n",
        "    # We compute the loss and update parameters\n",
        "    def learn(self):\n",
        "        # Get sampling range\n",
        "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
        "        # Randomly sample indices\n",
        "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
        "\n",
        "        # Convert to tensors\n",
        "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
        "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
        "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
        "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
        "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
        "\n",
        "        self.update(state_batch, action_batch, reward_batch, next_state_batch)\n",
        "\n",
        "\n",
        "# This update target parameters slowly\n",
        "# Based on rate `tau`, which is much less than one.\n",
        "@tf.function\n",
        "def update_target(target_weights, weights, tau):\n",
        "    for (a, b) in zip(target_weights, weights):\n",
        "        a.assign(b * tau + a * (1 - tau))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNuuWrTudwUg"
      },
      "source": [
        "def get_actor():\n",
        "    #last_init = tf.random_uniform_initializer(minval=1, maxval=2)\n",
        "\n",
        "    inputs = layers.Input(shape=(num_states,))\n",
        "    out = layers.Dense(256, activation=\"relu\")(inputs)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1, activation=\"tanh\")(out)\n",
        "    # Our upper bound is 2.0 for Pendulum.\n",
        "    outputs = outputs * upper_bound\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_critic():\n",
        "    # State as input\n",
        "    state_input = layers.Input(shape=(num_states))\n",
        "    state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
        "    state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
        "\n",
        "    # Action as input\n",
        "    action_input = layers.Input(shape=(num_actions))\n",
        "    action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
        "\n",
        "    # Both are passed through seperate layer before concatenating\n",
        "    concat = layers.Concatenate()([state_out, action_out])\n",
        "\n",
        "    out = layers.Dense(256, activation=\"relu\")(concat)\n",
        "    out = layers.Dense(256, activation=\"relu\")(out)\n",
        "    outputs = layers.Dense(1)(out)\n",
        "\n",
        "    # Outputs single value for give state-action\n",
        "    model = tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef9oKrRDdx0v"
      },
      "source": [
        "def policy(state, noise_object):\n",
        "    sampled_actions = tf.squeeze(actor_model(state))\n",
        "    noise = noise_object()\n",
        "    # Adding noise to action\n",
        "    sampled_actions = sampled_actions.numpy() + noise\n",
        "\n",
        "    # We make sure action is within bounds\n",
        "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
        "\n",
        "    return [np.squeeze(legal_action)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2bU7Fbqd0h6"
      },
      "source": [
        "s = 1 #km\n",
        "v = 80 #km/h\n",
        "t = int(min_time(s, v)*1.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_8zgJqZd1_e"
      },
      "source": [
        "train = Train(s_limit=s, time_limit=t, v_limit=v, dt = 1)\n",
        "train.reset()\n",
        "df_limit_speed = limit_plot(train.df_limit, return_df=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLedjd0yd3me"
      },
      "source": [
        "env = train\n",
        "\n",
        "num_states = 3\n",
        "print(\"Size of State Space ->  {}\".format(num_states))\n",
        "num_actions = 1\n",
        "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
        "\n",
        "upper_bound = env.action_space_high\n",
        "lower_bound = env.action_space_low\n",
        "\n",
        "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
        "print(\"Min Value of Action ->  {}\".format(lower_bound))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IdXKqBFd5Vq"
      },
      "source": [
        "std_dev = 0.2\n",
        "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
        "\n",
        "actor_model = get_actor()\n",
        "critic_model = get_critic()\n",
        "\n",
        "target_actor = get_actor()\n",
        "target_critic = get_critic()\n",
        "\n",
        "# Making the weights equal initially\n",
        "target_actor.set_weights(actor_model.get_weights())\n",
        "target_critic.set_weights(critic_model.get_weights())\n",
        "\n",
        "# Learning rate for actor-critic models\n",
        "critic_lr = 0.002\n",
        "actor_lr = 0.001\n",
        "\n",
        "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
        "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
        "\n",
        "total_episodes = 10000\n",
        "# Discount factor for future rewards\n",
        "gamma = 0.99\n",
        "# Used to update target networks\n",
        "tau = 0.005\n",
        "\n",
        "buffer = Buffer(batch_size=1000, buffer_capacity=int(1e6))\n",
        "\n",
        "ost = 100\n",
        "delta = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3bp3XLod65d"
      },
      "source": [
        "# To store reward history of each episode\n",
        "ep_reward_list = []\n",
        "# To store average reward history of last few episodes\n",
        "avg_reward_list = []\n",
        "\n",
        "# Takes about 4 min to train\n",
        "for ep in range(total_episodes):\n",
        "\n",
        "    prev_state = env.reset()\n",
        "    episodic_reward = 0\n",
        "    \n",
        "    while True:\n",
        "        # Uncomment this to see the Actor in action\n",
        "        # But not in a python notebook.\n",
        "        # env.render()\n",
        "\n",
        "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
        "\n",
        "        action = policy(tf_prev_state, ou_noise)[0]\n",
        "        # Recieve state and reward from environment.\n",
        "        state, reward, done = env.step(action)\n",
        "\n",
        "        buffer.record((prev_state, action, reward, state))\n",
        "        episodic_reward += reward\n",
        "\n",
        "        buffer.learn()\n",
        "        update_target(target_actor.variables, actor_model.variables, tau)\n",
        "        update_target(target_critic.variables, critic_model.variables, tau)\n",
        "\n",
        "        # End this episode when `done` is True\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        prev_state = state\n",
        "\n",
        "    ep_reward_list.append(episodic_reward)\n",
        "\n",
        "    # Mean of last 40 episodes\n",
        "    avg_reward = np.mean(ep_reward_list[-100:])\n",
        "    if ep % 100 == 0:\n",
        "        print(\"Episode * {} * Avg Reward is ==> {} Max reward ==> {}\".format(ep, avg_reward, max(ep_reward_list[-100:])))\n",
        "    avg_reward_list.append(avg_reward)\n",
        "    if reward == 100 and np.abs(ost) - np.abs(np.mean(ep_reward_list[-10:])) < delta:\n",
        "        print(\"Done! Episode * {} * Reward is ==> {}\".format(ep, episodic_reward))\n",
        "        break\n",
        "\n",
        "# Plotting graph\n",
        "# Episodes versus Avg. Rewards\n",
        "plt.plot(avg_reward_list)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrnK10IOd8XW"
      },
      "source": [
        "df = train.speed_legend()\n",
        "df.acceleration /= (3600**2 / 1000)\n",
        "plot_result(df)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}